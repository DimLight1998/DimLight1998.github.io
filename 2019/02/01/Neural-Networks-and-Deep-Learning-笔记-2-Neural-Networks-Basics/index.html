<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang="zh-CN">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222">
<meta name="google-site-verification" content="mS6tdIKrisqbP1HemlJXaAUWsqoLxTRprQdjQnk-OYk" />












<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.4.0" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.4.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.4.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.4.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.4.0" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '6.4.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="Neural Networks BasicsLogistic Regression as a Neural NetworkBinary Classification课程中使用的符号：  $m$ 表示数据集大小，$m_{train}$ 和 $m_{test}$ 分别表示训练集和测试集的大小。 $n$ 表示每个样本的特征向量的维数。  $X$ 表示整个训练集，每一列表示一个样本，相当于样本按照列进行堆">
<meta name="keywords" content="机器学习,深度学习">
<meta property="og:type" content="article">
<meta property="og:title" content="Neural Networks and Deep Learning 笔记 2 Neural Networks Basics">
<meta property="og:url" content="http://DimLight1998.github.io/2019/02/01/Neural-Networks-and-Deep-Learning-笔记-2-Neural-Networks-Basics/index.html">
<meta property="og:site_name" content="Computation &amp; Design">
<meta property="og:description" content="Neural Networks BasicsLogistic Regression as a Neural NetworkBinary Classification课程中使用的符号：  $m$ 表示数据集大小，$m_{train}$ 和 $m_{test}$ 分别表示训练集和测试集的大小。 $n$ 表示每个样本的特征向量的维数。  $X$ 表示整个训练集，每一列表示一个样本，相当于样本按照列进行堆">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2019-02-28T08:20:27.745Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Neural Networks and Deep Learning 笔记 2 Neural Networks Basics">
<meta name="twitter:description" content="Neural Networks BasicsLogistic Regression as a Neural NetworkBinary Classification课程中使用的符号：  $m$ 表示数据集大小，$m_{train}$ 和 $m_{test}$ 分别表示训练集和测试集的大小。 $n$ 表示每个样本的特征向量的维数。  $X$ 表示整个训练集，每一列表示一个样本，相当于样本按照列进行堆">






  <link rel="canonical" href="http://DimLight1998.github.io/2019/02/01/Neural-Networks-and-Deep-Learning-笔记-2-Neural-Networks-Basics/"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Neural Networks and Deep Learning 笔记 2 Neural Networks Basics | Computation & Design</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Computation & Design</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br />首页</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">
    <a href="/tags/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />标签</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />归档</a>
  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://DimLight1998.github.io/2019/02/01/Neural-Networks-and-Deep-Learning-笔记-2-Neural-Networks-Basics/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhang Yang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Computation & Design">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Neural Networks and Deep Learning 笔记 2 Neural Networks Basics
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-02-01 21:49:49" itemprop="dateCreated datePublished" datetime="2019-02-01T21:49:49+08:00">2019-02-01</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-02-28 16:20:27" itemprop="dateModified" datetime="2019-02-28T16:20:27+08:00">2019-02-28</time>
              
            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/02/01/Neural-Networks-and-Deep-Learning-笔记-2-Neural-Networks-Basics/#comments" itemprop="discussionUrl">
                
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2019/02/01/Neural-Networks-and-Deep-Learning-笔记-2-Neural-Networks-Basics/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Neural-Networks-Basics"><a href="#Neural-Networks-Basics" class="headerlink" title="Neural Networks Basics"></a>Neural Networks Basics</h1><h2 id="Logistic-Regression-as-a-Neural-Network"><a href="#Logistic-Regression-as-a-Neural-Network" class="headerlink" title="Logistic Regression as a Neural Network"></a>Logistic Regression as a Neural Network</h2><h3 id="Binary-Classification"><a href="#Binary-Classification" class="headerlink" title="Binary Classification"></a>Binary Classification</h3><p>课程中使用的符号：</p>
<ul>
<li>$m$ 表示数据集大小，$m_{train}$ 和 $m_{test}$ 分别表示训练集和测试集的大小。</li>
<li><p>$n$ 表示每个样本的特征向量的维数。</p>
</li>
<li><p>$X$ 表示整个训练集，<strong>每一列表示一个样本</strong>，相当于样本按照列进行堆叠，因此 $X$ 的规模是 $n\times m$ 的，据说这样表示更简单。</p>
</li>
<li>$Y$ 表示所有的标签，每一列表示一个样本，因此 $Y$ 的规模是 $1 \times m$ 的。</li>
</ul>
<h3 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h3><p>对于一个样本特征，逻辑回归用于得到这个样本对应的输出，不过这个输出被限制在了 0 到 1 的范围内，因此可以用来做一些类似二分类的事情。比如现在要对某个特征向量 $\vec x$ 进行二分类，希望得到一个输出 $\hat y$ 表示在当前特征为 $\vec x$ 的情况下真实输出为 1 的概率（二分类时这个输出是离散的，只有 0 和 1 两种情况，但是概率是连续的），可以选择 $\hat y = \vec w^T\vec x + b$ 这个模型（这时是线性回归），但是这样得到的值不一定在 0 到 1 之间，因此可以套上一个 sigmoid 函数 $\displaystyle{\sigma(x) = \frac{1}{1 + e^{-x}}}$，选择模型 $\hat y = \sigma(\vec w^T\vec x + b)$，此时为逻辑回归。</p>
<h3 id="Logistic-Regression-Cost-Function"><a href="#Logistic-Regression-Cost-Function" class="headerlink" title="Logistic Regression Cost Function"></a>Logistic Regression Cost Function</h3><p>为了找到逻辑回归中最好的 $\vec w$ 和 $b$，需要一个度量回归质量好坏的标准。对于单个的样本可以使用 <strong>loss function</strong> 完成这件事，使用 $L(\hat y, y)$ 描述二者之间的相近程度，这个值越小越好。尽管使用 $L(\hat y, y) = \frac{1}{2}(y - \hat y)^2$ 可以进行度量（称为平方误差），但是优化的时候这个定义会导致有多个局部最优解，故逻辑回归中一般不使用这种 loss function。一般选用的是 $L(\hat y, y) = -(y\log \hat y+ (1 - y)\log (1 - \hat y))$，当最小化这个函数的时候 $\hat y$ 会尽可能与 $y$ 相近（考虑到 $y$ 非 0 则 1，而 $\hat y$ 只可能在 0 到 1 之间）。</p>
<p>上述的度量是针对一个样本的，如果要对多个样本的回归质量进行度量要用到 <strong>cost function</strong>，符号是 $J(\vec w, b)$（注意自变量不是 $\hat y$ 和 $y$ 了）。它实际上就是多个样本的 loss function 的算术平均：</p>
<script type="math/tex; mode=display">
J(\vec w, b) = \frac{1}{m}\sum\limits_{i = 1}^m L(\hat y_i, y_i) = -\frac{1}{m}\sum\limits_{i = 1}^m(y_i\log \hat y_i+ (1 - y_i)\log (1 - \hat y_i))</script><p>逻辑回归就是要找到最好的 $\vec w$ 和 $b$ 以最小化整个样本上的 cost function。</p>
<p>（中文翻译把 loss function 翻译为损失函数，cost function 翻译为代价函数。）</p>
<p>逻辑回归可以看成一个小的神经网络。</p>
<h3 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h3><p>为了最小化代价函数需要用到梯度下降，重复以下过程直到 $\vec w$ 和 $b$ 收敛至结果令人满意：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\vec w &:=\vec w - \alpha\frac{\partial J(\vec w, b)}{\partial \vec w} \\
b&:=b -\alpha\frac{\partial J(\vec w, b)}{\partial b}
\end{aligned}</script><p>其中 $\alpha $ 称作学习率（大于零），它控制每次迭代时更新各参数的步长。可以注意到在很接近最优解的时候偏导可能很小，此时步长变得很小。</p>
<p>最后一件事是对 $\vec w$ 和 $b$ 选择初始值。在逻辑回归中，因为按照上述定义得到的代价函数只会有一个最优解，所以不管怎么样的初始化都会导致一样的结果，所以全部初始化为 0 就可以了。</p>
<h3 id="Derivatives"><a href="#Derivatives" class="headerlink" title="Derivatives"></a>Derivatives</h3><p><em>本节没有问题。</em></p>
<h3 id="More-Derivative-Examples"><a href="#More-Derivative-Examples" class="headerlink" title="More Derivative Examples"></a>More Derivative Examples</h3><p><em>本节没有问题。</em></p>
<h3 id="Computation-graph"><a href="#Computation-graph" class="headerlink" title="Computation graph"></a>Computation graph</h3><p>一个式子可以被组织成 <strong>计算图</strong> 的形式，计算图是一张有向无环图。它有一些没有前驱的结点，一般是各个变量；还有一个没有后继的结点，一般是输出值。前向计算这张计算图可以得到输出，如果反向的话就可以得到输出关于各个输入变量的导数。对于优化而言能够求得导数是很有用的。</p>
<h3 id="Derivatives-with-a-Computation-Graph"><a href="#Derivatives-with-a-Computation-Graph" class="headerlink" title="Derivatives with a Computation Graph"></a>Derivatives with a Computation Graph</h3><p>计算图中，最终输出的结果对某个输入变量的导数使用链式法则来求。通过计算图的前驱后继关系逆着往回一级一级地求即可。</p>
<h3 id="Logistic-Regression-Gradient-Descent"><a href="#Logistic-Regression-Gradient-Descent" class="headerlink" title="Logistic Regression Gradient Descent"></a>Logistic Regression Gradient Descent</h3><p>这节使用计算图对单个数据样本的情形求出损失函数对各系数的导数。</p>
<p>首先使用计算图进行建模，建立计算图时不一定要分解到最原子的操作，按照逻辑进行分解即可。输入是特征 $\vec x$ 的 $n$ 个分量，$\vec w$ 的 $n$ 个分量以及 $b$ 一共 $2n + 1$ 个结点，然后计算 $z = b + \sum\limits_{i = 1}^n \vec x_i\cdot \vec w_i$，然后计算 $\hat y = \sigma(z)$，然后计算损失函数 $L(\hat y, y) = -(y\log \hat y+ (1 - y)\log (1 - \hat y))$。求导得到</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial z}{\partial b} &= 1, \frac{\partial z}{\partial \vec x_i} = \vec w_i \\
\frac{\partial \hat y}{\partial z} &=\frac{e^z}{\left(e^z+1\right)^2} = \hat y(1 - \hat y) \\
\frac{\partial L}{\partial \hat y} &= -\frac{y-1}{1-\hat y}-\frac{y}{\hat y} \\
\end{aligned}</script><p>乘起来得到</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial L}{\partial b} &= \frac{\partial L}{\partial \hat y}\cdot\frac{\partial\hat y}{\partial z}\cdot\frac{\partial z}{\partial b} \\
&= \left(-\frac{y-1}{1-\hat y}-\frac{y}{\hat y}\right)\hat y(1- \hat y) \\
&= \hat y - y \\
&= \sigma\left(b + \sum\limits_{i = 1}^n \vec x_i\cdot \vec w_i\right) - y \\

\frac{\partial L}{\partial \vec w_i} &= \frac{\partial L}{\partial \hat y}\cdot\frac{\partial\hat y}{\partial z}\cdot\frac{\partial z}{\partial \vec w_i} \\
&= \left(-\frac{y-1}{1-\hat y}-\frac{y}{\hat y}\right)\hat y(1- \hat y) \vec x_i\\
&= (\hat y - y) \vec x_i \\
&= \left(\sigma\left(b + \sum\limits_{j = 1}^n \vec x_j\cdot \vec w_j\right) - y\right)\vec x_i \\
\end{aligned}</script><p>把 sigmoid 函数的导数写成因变量的函数很有用。</p>
<h3 id="Gradient-Descent-on-m-Examples"><a href="#Gradient-Descent-on-m-Examples" class="headerlink" title="Gradient Descent on $m$ Examples"></a>Gradient Descent on $m$ Examples</h3><p>前一节是针对一个样本的情形。在有 $m$ 个样本的情况下，要优化的是代价函数 $J(\vec w, b)$，它实际上是各个损失函数的算术平均，所以有</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial J}{\partial L_i} &= \frac{1}{m} \\
\end{aligned}</script><p>于是</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial J}{\partial b} &= \sum\limits_{j = 1}^{m}\left(\frac{\partial J}{\partial L_j}\cdot\frac{\partial L_j}{\partial \hat y_j}\cdot\frac{\partial\hat y_j}{\partial z_j}\cdot\frac{\partial z_j}{\partial b}\right) \\
&= \frac{1}{m}\sum\limits_{j = 1}^{m}\left(\left(-\frac{y_j-1}{1-\hat y_j}-\frac{y_j}{\hat y_j}\right)\hat y_j(1- \hat y_j)\right) \\
&= \frac{1}{m}\sum\limits_{j = 1}^{m}\left(\hat y_j - y_j\right)\\
&= \frac{1}{m}\sum\limits_{j = 1}^{m}\left(\sigma\left(b + \sum\limits_{i = 1}^n \vec x_{ji}\cdot \vec w_i\right) - y_j \right)\\
\end{aligned}</script><p>对 $w_i$ 的求导类似。</p>
<p>在使用代码实现时需要用到多个循环（对特征的一次循环、对样本的一次循环、对梯度下降的一次循环，至少三次），使用 <strong>向量化</strong> 的技术可以减少循环的出现次数，同时提高运行效率。</p>
<h2 id="Python-and-Vectorization"><a href="#Python-and-Vectorization" class="headerlink" title="Python and Vectorization"></a>Python and Vectorization</h2><h3 id="Vectorization"><a href="#Vectorization" class="headerlink" title="Vectorization"></a>Vectorization</h3><p>两个向量的内积可以使用 <code>numpy</code> 的 <code>dot</code> 函数完成，替代掉更慢的 for 循环。更快的原因是 CPU 会利用一些 SIMD 的特性加速运算。使用 GPU 对这种加速更加显著。</p>
<h3 id="More-Vectorization-Examples"><a href="#More-Vectorization-Examples" class="headerlink" title="More Vectorization Examples"></a>More Vectorization Examples</h3><p>一个矩阵左乘一个向量可以使用 <code>dot</code> 函数完成。</p>
<p>如果需要对矩阵/向量中的元素做逐元素的运算，有 <code>np.exp</code>、<code>np.log</code>、<code>np.maximum</code> 等函数可以使用。</p>
<h3 id="Vectorizing-Logistic-Regression"><a href="#Vectorizing-Logistic-Regression" class="headerlink" title="Vectorizing Logistic Regression"></a>Vectorizing Logistic Regression</h3><p>在逻辑回归的实现中利用向量化可以在不使用任何 for 语句的情况下计算整个数据集数据的输出。</p>
<p>用 <code>w</code> 表示 $\vec w$，<code>b</code> 表示 $b$，<code>xs</code> 表示整个数据集（规模应为 $n\times m$），那么对输出的中间计算可以直接写成 <code>np.dot(w.T, xs) + b</code>。其中用到了 numpy 中 <strong>广播</strong> 的特性，使得 <code>np.dot(x.T, xs)</code> 和 <code>b</code> 这两个规模不同的量可以相加。随后也可以使用向量化的方法对中间结果逐元素施加 sigmoid 函数。</p>
<h3 id="Vectorizing-Logistic-Regression’s-Gradient-Output"><a href="#Vectorizing-Logistic-Regression’s-Gradient-Output" class="headerlink" title="Vectorizing Logistic Regression’s Gradient Output"></a>Vectorizing Logistic Regression’s Gradient Output</h3><p>在逻辑回归的实现中使用向量化也能在反向计算导数时进行加速。</p>
<p>最后的实现应该只包含对迭代次数的 for 循环，其它所有的循环都应该能够被向量化。</p>
<h3 id="Broadcasting-in-Python"><a href="#Broadcasting-in-Python" class="headerlink" title="Broadcasting in Python"></a>Broadcasting in Python</h3><p>Python 中的广播特性可以让两个不同规模的矩阵进行原来要相同规模的矩阵才能进行的运算，不过这两个不同的规模也要满足一定的条件。</p>
<p>一些简单的原则：</p>
<ul>
<li>一个 $m\times n$ 的矩阵和 $m\times 1$ 的矩阵做二元运算时，$m\times 1$ 矩阵会按列重复 $n$ 次，形成 $m\times n$ 矩阵。</li>
<li>一个 $m\times n$ 的矩阵和 $1\times n$ 的矩阵做二元运算时，$1 \times n$ 矩阵会按行重复 $m$ 次，形成 $m\times n$ 矩阵。</li>
<li>一个列向量或者行向量和实数做二元运算时，实数会自动重复到和向量的规模一致。</li>
</ul>
<h3 id="A-note-on-python-numpy-vectors"><a href="#A-note-on-python-numpy-vectors" class="headerlink" title="A note on python/numpy vectors"></a>A note on python/numpy vectors</h3><p>这一节主要是对使用 numpy 时的一些建议。</p>
<p>建议不要使用 rank 为 1 的数组，这些数组既不是列向量也不是行向量，应该避免使用。总是显式地使用 $1\times n$ 或者 $n\times 1$ 的矩阵当做数学中的向量。</p>
<p>代码中可以考虑多写一些关于 shape 的断言语句。</p>
<h3 id="Quick-tour-of-Jupyter-iPython-Notebooks"><a href="#Quick-tour-of-Jupyter-iPython-Notebooks" class="headerlink" title="Quick tour of Jupyter/iPython Notebooks"></a>Quick tour of Jupyter/iPython Notebooks</h3><p><em>本节没有问题。</em></p>
<h3 id="Explanation-of-logistic-regression-cost-function"><a href="#Explanation-of-logistic-regression-cost-function" class="headerlink" title="Explanation of logistic regression cost function"></a>Explanation of logistic regression cost function</h3><p>为什么逻辑回归的损失函数和代价函数的形式是那个样子的？损失函数 $\hat y$ 实际上是给定了 $\vec x$ 的情况下 $y = 1$ 的概率，即 $\hat y = P(y = 1 | \vec x)$，此外 $1 - \hat y = P(y = 0 | \vec x)$。这可以写成：</p>
<script type="math/tex; mode=display">
\begin{aligned}
y = 1 &\Rightarrow P(y|\vec x) = \hat y \\
y = 0 &\Rightarrow P(y|\vec x) = 1 - \hat y
\end{aligned}</script><p>两种情况下，要最大化的都是 $P(y |\vec x)$，而它可以写成如下形式以统一 $y$：</p>
<script type="math/tex; mode=display">
P(y | \vec x) = \hat y^{y}(1 - \hat y)^{1 - y}</script><p>这是要最大化的目标，给它套一个对数函数，要最大化的目标就变成了</p>
<script type="math/tex; mode=display">
\log P(y|\hat x) = y\log \hat y + (1 - y)\log (1 - \hat y)</script><p>损失函数是这个函数取反，最大化这个目标就是最小化损失函数。</p>
<p>以上是损失函数的情况，对于多个样本的代价函数，在样本独立同分布的情况下，最大化的目标是</p>
<script type="math/tex; mode=display">
P(y_1|\vec x_1)\cdots P(y_m|\vec x_m)</script><p>即</p>
<script type="math/tex; mode=display">
\begin{aligned}
 & \log\left(P(y_1|\vec x_1)\cdots P(y_m|\vec x_m)\right) \\
=& \log\prod\limits_{i = 1}^mP(y_i|\vec x_i) \\
=& \sum\limits_{i = 1}^m\log P(y_i|\vec x_i) \\
=& \sum\limits_{i = 1}^m\left(y_i\log\hat y_i + (1 - y_i)\log(1 - \hat y_i)\right) \\
=& -\sum\limits_{i = 1}^mL(\hat y_i, y_i) \\
=& -mJ(\vec w, b)
\end{aligned}</script><p>最大化目标概率就是最小化代价函数 $J(\vec w, b)$。这是一个最大似然估计的过程，要选择最佳的 $\vec w$ 和 $b$ 使概率最大。</p>
<p>​    </p>

      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag"># 机器学习</a>
          
            <a href="/tags/深度学习/" rel="tag"># 深度学习</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/02/01/Neural-Networks-and-Deep-Learning-笔记-1-Introduction-to-Deep-Learning/" rel="next" title="Neural Networks and Deep Learning 笔记 1 Introduction to Deep Learning">
                <i class="fa fa-chevron-left"></i> Neural Networks and Deep Learning 笔记 1 Introduction to Deep Learning
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/02/01/Neural-Networks-and-Deep-Learning-笔记-3-Shallow-Neural-Networks/" rel="prev" title="Neural Networks and Deep Learning 笔记 3 Shallow Neural Networks">
                Neural Networks and Deep Learning 笔记 3 Shallow Neural Networks <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Zhang Yang</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">89</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">17</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          
          

          
          

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Neural-Networks-Basics"><span class="nav-number">1.</span> <span class="nav-text">Neural Networks Basics</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Logistic-Regression-as-a-Neural-Network"><span class="nav-number">1.1.</span> <span class="nav-text">Logistic Regression as a Neural Network</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Binary-Classification"><span class="nav-number">1.1.1.</span> <span class="nav-text">Binary Classification</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Logistic-Regression"><span class="nav-number">1.1.2.</span> <span class="nav-text">Logistic Regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Logistic-Regression-Cost-Function"><span class="nav-number">1.1.3.</span> <span class="nav-text">Logistic Regression Cost Function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Gradient-Descent"><span class="nav-number">1.1.4.</span> <span class="nav-text">Gradient Descent</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Derivatives"><span class="nav-number">1.1.5.</span> <span class="nav-text">Derivatives</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#More-Derivative-Examples"><span class="nav-number">1.1.6.</span> <span class="nav-text">More Derivative Examples</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Computation-graph"><span class="nav-number">1.1.7.</span> <span class="nav-text">Computation graph</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Derivatives-with-a-Computation-Graph"><span class="nav-number">1.1.8.</span> <span class="nav-text">Derivatives with a Computation Graph</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Logistic-Regression-Gradient-Descent"><span class="nav-number">1.1.9.</span> <span class="nav-text">Logistic Regression Gradient Descent</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Gradient-Descent-on-m-Examples"><span class="nav-number">1.1.10.</span> <span class="nav-text">Gradient Descent on $m$ Examples</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Python-and-Vectorization"><span class="nav-number">1.2.</span> <span class="nav-text">Python and Vectorization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Vectorization"><span class="nav-number">1.2.1.</span> <span class="nav-text">Vectorization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#More-Vectorization-Examples"><span class="nav-number">1.2.2.</span> <span class="nav-text">More Vectorization Examples</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Vectorizing-Logistic-Regression"><span class="nav-number">1.2.3.</span> <span class="nav-text">Vectorizing Logistic Regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Vectorizing-Logistic-Regression’s-Gradient-Output"><span class="nav-number">1.2.4.</span> <span class="nav-text">Vectorizing Logistic Regression’s Gradient Output</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Broadcasting-in-Python"><span class="nav-number">1.2.5.</span> <span class="nav-text">Broadcasting in Python</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#A-note-on-python-numpy-vectors"><span class="nav-number">1.2.6.</span> <span class="nav-text">A note on python/numpy vectors</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Quick-tour-of-Jupyter-iPython-Notebooks"><span class="nav-number">1.2.7.</span> <span class="nav-text">Quick tour of Jupyter/iPython Notebooks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Explanation-of-logistic-regression-cost-function"><span class="nav-number">1.2.8.</span> <span class="nav-text">Explanation of logistic regression cost function</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhang Yang</span>

  

  
</div>




  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动 v3.7.1</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a class="theme-link" target="_blank" href="https://theme-next.org">NexT.Muse</a> v6.4.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    
	
    

    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.4.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.4.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.4.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.4.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.4.0"></script>



  

  
    <script id="dsq-count-scr" src="https://dimlight1998.disqus.com/count.js" async></script>
  

  
    <script type="text/javascript">
      var disqus_config = function () {
        this.page.url = 'http://DimLight1998.github.io/2019/02/01/Neural-Networks-and-Deep-Learning-笔记-2-Neural-Networks-Basics/';
        this.page.identifier = '2019/02/01/Neural-Networks-and-Deep-Learning-笔记-2-Neural-Networks-Basics/';
        this.page.title = 'Neural Networks and Deep Learning 笔记 2 Neural Networks Basics';
        };
      function loadComments () {
        var d = document, s = d.createElement('script');
        s.src = 'https://dimlight1998.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      }
      
        loadComments();
      
    </script>
  












  





  

  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  


  
  

  

  

  

  

  

</body>
</html>

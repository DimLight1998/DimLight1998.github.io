<!DOCTYPE html>












  


<html class="theme-next mist use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">























  
  
  
  

  

  

  

  

  

  






<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=7.0.1">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.0.1">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.0.1">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.0.1">


  <link rel="mask-icon" href="/images/logo.svg?v=7.0.1" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '7.0.1',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="Recurrent Neural Networks来源 Coursera-Sequence Models. Recurrent Neural NetworksWhy sequence models一些序列模型的应用：  Speech recognition Music generation Sentiment classfication DNA Sequence analysis Machine">
<meta name="keywords" content="机器学习,深度学习,RNN">
<meta property="og:type" content="article">
<meta property="og:title" content="Sequence Models 笔记 1 Recurrent Neural Networks">
<meta property="og:url" content="http://DimLight1998.github.io/2019/03/23/Sequence-Models-笔记-1-Recurrent-Neural-Networks/index.html">
<meta property="og:site_name" content="Computation &amp; Design">
<meta property="og:description" content="Recurrent Neural Networks来源 Coursera-Sequence Models. Recurrent Neural NetworksWhy sequence models一些序列模型的应用：  Speech recognition Music generation Sentiment classfication DNA Sequence analysis Machine">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://dimlight1998.github.io/2019/03/23/Sequence-Models-笔记-1-Recurrent-Neural-Networks/DeepinScreenshot_select-area_20190328210024.png">
<meta property="og:updated_time" content="2019-04-03T03:12:31.770Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Sequence Models 笔记 1 Recurrent Neural Networks">
<meta name="twitter:description" content="Recurrent Neural Networks来源 Coursera-Sequence Models. Recurrent Neural NetworksWhy sequence models一些序列模型的应用：  Speech recognition Music generation Sentiment classfication DNA Sequence analysis Machine">
<meta name="twitter:image" content="http://dimlight1998.github.io/2019/03/23/Sequence-Models-笔记-1-Recurrent-Neural-Networks/DeepinScreenshot_select-area_20190328210024.png">






  <link rel="canonical" href="http://DimLight1998.github.io/2019/03/23/Sequence-Models-笔记-1-Recurrent-Neural-Networks/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Sequence Models 笔记 1 Recurrent Neural Networks | Computation & Design</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Computation & Design</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
        </li>
      
    </ul>
  

  
    

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://DimLight1998.github.io/2019/03/23/Sequence-Models-笔记-1-Recurrent-Neural-Networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhang Yang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Computation & Design">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Sequence Models 笔记 1 Recurrent Neural Networks

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-23 23:43:52" itemprop="dateCreated datePublished" datetime="2019-03-23T23:43:52+08:00">2019-03-23</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-04-03 11:12:31" itemprop="dateModified" datetime="2019-04-03T11:12:31+08:00">2019-04-03</time>
              
            
          </span>

          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Recurrent-Neural-Networks"><a href="#Recurrent-Neural-Networks" class="headerlink" title="Recurrent Neural Networks"></a>Recurrent Neural Networks</h1><p>来源 <a href="https://www.coursera.org/learn/nlp-sequence-models" target="_blank" rel="noopener">Coursera-Sequence Models</a>.</p>
<h2 id="Recurrent-Neural-Networks-1"><a href="#Recurrent-Neural-Networks-1" class="headerlink" title="Recurrent Neural Networks"></a>Recurrent Neural Networks</h2><h3 id="Why-sequence-models"><a href="#Why-sequence-models" class="headerlink" title="Why sequence models"></a>Why sequence models</h3><p>一些序列模型的应用：</p>
<ul>
<li>Speech recognition</li>
<li>Music generation</li>
<li>Sentiment classfication</li>
<li>DNA Sequence analysis</li>
<li>Machine translation</li>
<li>Video activity recognition</li>
<li>Name entity recognition（从文本序列中标记出表示名称的单词，例如人名）</li>
</ul>
<h3 id="Notation"><a href="#Notation" class="headerlink" title="Notation"></a>Notation</h3><h4 id="序列相关的记号"><a href="#序列相关的记号" class="headerlink" title="序列相关的记号"></a>序列相关的记号</h4><p>因为小括号、中括号和大括号都被占用了，这里使用尖括号对序列中的元素进行索引。如果 $x$ 是一个样本，它是一个序列，那么它的第 $t$ 个元素是 $x^{\langle k\rangle }$。</p>
<p>使用 $T_x$ 表示 $x$ 的长度。</p>
<h4 id="序列的表示"><a href="#序列的表示" class="headerlink" title="序列的表示"></a>序列的表示</h4><p>以文本序列为例，首先准备一个词典，其中包含 <code>&lt;UNK&gt;</code> 表示不在词典中的所有词。每个词都有自己的唯一序号。对于一个序列 $x$，将其中的每个单词都编码为一个 one hot 的向量，其中只有单词在词典中的序号对应的位置是 1，其余位置是零。这样就把一个单词序列编码为了 one hot 向量的序列。</p>
<h3 id="Recurrent-Neural-Network-Model"><a href="#Recurrent-Neural-Network-Model" class="headerlink" title="Recurrent Neural Network Model"></a>Recurrent Neural Network Model</h3><h4 id="为何不使用普通网络"><a href="#为何不使用普通网络" class="headerlink" title="为何不使用普通网络"></a>为何不使用普通网络</h4><ul>
<li>普通神经网络在处理序列问题时无法考虑到时间上的局部信息，序列中的每个元素在其看来是平权的。</li>
<li>出于和引入 CNN 一样的原因，使用 RNN 可以学习到如何对临近的元素提取信息，并可以减小参数数目。</li>
<li>序列的长度不是固定的，普通神经网络无法处理这个问题。</li>
</ul>
<h4 id="递归神经网络"><a href="#递归神经网络" class="headerlink" title="递归神经网络"></a>递归神经网络</h4><p>RNN 可以将一个序列映射到另一个序列。<strong>在整个序列映射过程中，网络参数保持不变。</strong></p>
<p>对于元素 $x^{\langle t\rangle }$，它对应的 $\hat y^{\langle t\rangle }$ 既取决于输入 $x^{\langle t\rangle }$ 也取决于上次的激活值 $a^{\langle t- 1\rangle }$。在计算过程中，$x^{\langle t\rangle }$ 既产生预测值 $\hat y^{\langle t\rangle }$ 也产生激活值 $a^{\langle t\rangle }$。初始时的 $a^{\langle 0\rangle }$ 可以是零向量，也可以是某个随机值。</p>
<h4 id="简单递归神经网络的问题"><a href="#简单递归神经网络的问题" class="headerlink" title="简单递归神经网络的问题"></a>简单递归神经网络的问题</h4><p>上述 RNN 的问题在于对于序列中的某个元素只能利用序列中该元素前的信息，而不能利用后续信息。这在一些应用中是不利的，但是有解决方案。</p>
<h4 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h4><p>RNN 中的一个模块需要维护三个权值和两个偏置：$W_{aa}, W_{ax}, W_{ya}, b_a, b_y$。<em>命名方式：第一个角标指示用来计算什么，第二个角标如果存在依据什么计算。</em></p>
<p>对于上次的激活值 $a^{\langle t - 1\rangle }$ 以及这次的输入 $x^{\langle t\rangle }$，有</p>
<script type="math/tex; mode=display">
\begin{aligned}
a^{\langle t\rangle } &= g_1(W_{aa}a^{\langle t-1\rangle } + W_{ax}x^{\langle t\rangle } + b_a) \\
y^{\langle t\rangle } &= g_2(W_{ya}a^{\langle t\rangle } + b_y)
\end{aligned}</script><p>可以进一步简化这个符号。如果将 $W_{aa}$ 和 $W_{ax}$ 横向拼接起来得到 $W_a$，将 $W_{ya}$ 记作 $W_y$，将 $a^{\langle t - 1\rangle }$ 和 $x^{\langle t\rangle }$ 纵向拼接起来得到 $[a^{\langle t - 1\rangle }, x^{\langle t\rangle }]$，那么有</p>
<script type="math/tex; mode=display">
\begin{aligned}
a^{\langle t\rangle } &= g_1(W_{a}[a^{\langle t-1\rangle }, x^{\langle t\rangle }] + b_a) \\
y^{\langle t\rangle } &= g_2(W_{y}a^{\langle t\rangle } + b_y)
\end{aligned}</script><p>现在只要维护两个权值和偏置即可。<strong>再次注意对同一个序列而言，各元素计算时使用的参数是相同且共享的。</strong></p>
<a id="more"></a>
<h3 id="Backpropagation-through-time"><a href="#Backpropagation-through-time" class="headerlink" title="Backpropagation through time"></a>Backpropagation through time</h3><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>对于序列中的每个元素的输出可以定义损失函数 $\mathcal L(\hat y^{\langle t\rangle }, y^{\langle t\rangle })​$，将每个元素的损失函数加起来就得到了序列的损失函数 $\mathcal L(\hat y, y) = \sum\limits_{t = 1}^{T_y}\mathcal L(\hat y^{\langle t\rangle }, y^{\langle t\rangle })​$。</p>
<h4 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h4><p>对于一个序列，如果将计算过程中各个值的依赖关系有有向图表示出来，会得到一张 DAG。其中输入包括 $a^{\langle 0\rangle }, W_a, b_a, W_y, b_y, x^{\langle 1\rangle }, x^{\langle 2\rangle }, \dots, x^{\langle T_x\rangle }$，输出只有 $\mathcal L(\hat y, y)$。理论上，只要是 DAG 就可以对变量 $W_a, b_a, W_y, b_y$（或者包括 $a^{\langle 0\rangle }$）求导，然后梯度下降更新；实现上也没太大难度。</p>
<h3 id="Different-types-of-RNNs"><a href="#Different-types-of-RNNs" class="headerlink" title="Different types of RNNs"></a>Different types of RNNs</h3><p>有多种输入输出规模不同的 RNN。</p>
<h4 id="Many-to-many"><a href="#Many-to-many" class="headerlink" title="Many-to-many"></a>Many-to-many</h4><p>这种 RNN 就是上面写的。接受输入序列 $x​$ 返回输出序列 $\hat y$，并且满足 $T_x = T_y​$。</p>
<h4 id="Many-to-one"><a href="#Many-to-one" class="headerlink" title="Many-to-one"></a>Many-to-one</h4><p>接受输入序列 $x​$，但是只生成一个元素 $\hat y​$。典型的应用例如将文本评价转换为打分。结构上，可以认为和 many-to-many 的 RNN 类似，但是输出序列中除了 $\hat y^{\langle T_y\rangle }​$ 以外的元素都被忽略掉了。</p>
<h4 id="One-to-one"><a href="#One-to-one" class="headerlink" title="One-to-one"></a>One-to-one</h4><p>这种是 RNN 的退化情况，实际上就是简单的全连接网络。</p>
<h4 id="One-to-many"><a href="#One-to-many" class="headerlink" title="One-to-many"></a>One-to-many</h4><p>这种可以用来做序列生成，接受一个可以看做是种子的元素 $x​$ 生成序列 $\hat y​$。和普通的 RNN 类似，但是在后面的计算中因为缺失除了 $x^{\langle 1\rangle }​$ 以外的输入 ，于是使用 $\hat y^{\langle t - 1\rangle }​$ 当做 $x^{\langle t\rangle }​$。</p>
<h4 id="Many-to-many-with-different-length"><a href="#Many-to-many-with-different-length" class="headerlink" title="Many-to-many with different length"></a>Many-to-many with different length</h4><p>很多应用中，输入序列的长度和输出序列的长度不是相等的，例如机器翻译。为了实现这一点，可以将一个 many-to-one 当做 encoder，将一个 one-to-many 当做 decoder，然后将二者连接起来，就实现了输入输出不等长的 many-to-many。</p>
<h3 id="Language-model-and-sequence-generation"><a href="#Language-model-and-sequence-generation" class="headerlink" title="Language model and sequence generation"></a>Language model and sequence generation</h3><h4 id="什么是语言建模"><a href="#什么是语言建模" class="headerlink" title="什么是语言建模"></a>什么是语言建模</h4><p>对语言建模后，语言模型可以对一个句子（单词的序列）给出这个句子在该语言下出现的概率。如果得分较高，表明这个句子是合理的，反之是不合理的。</p>
<h4 id="如何对语言建模"><a href="#如何对语言建模" class="headerlink" title="如何对语言建模"></a>如何对语言建模</h4><p>首先准备很多句子的语料库用来进行训练。对于任意一个句子，它是单词的序列，记作 $x$。首先对句子拆分为 token。现在序列中要么是表示词典中已有单词的 token，要么是表示单词未知的 <code>&lt;UNK&gt;</code>，要么是表示句子结束的 <code>&lt;EOS&gt;</code>。然后对 token 进行 one-hot 编码。现在 $x$ 应当是 one-hot 向量的序列。</p>
<p>使用 RNN 进行训练。根据 $x$ 构建这样的输入序列 $x&#39;$：$x&#39;^{\langle 1\rangle } =  \vec 0$，$x&#39;^{\langle t\rangle } = x^{\langle t - 1\rangle }$，使得 $x&#39;$ 有 $T_x$ 个元素。如此一来 $x&#39;$ 中没有 <code>&lt;EOS&gt;</code>。然后让 $x$ 作为目标序列。训练这个 RNN 即可。</p>
<p><strong>有效的原因：</strong>考虑序列中的第一个元素，输入为 $x&#39;^{\langle 1\rangle } = \vec 0$，输出为 $\hat y^{\langle 1\rangle }$，这个输出是一个经过了 softmax 的表示句子中第一个单词是 $x^{\langle 1\rangle }$ 对应单词的概率，即 $P(x^{\langle 1\rangle })$。考虑第二个元素，输入为 $x&#39;^{\langle 2\rangle } = x^{\langle 1\rangle }$，输出表示的是在已知第一个单词是 $x^{\langle 1\rangle }$ 的情况下，句子的第二个单词是 $x^{\langle 2\rangle }$ 的概率，即 $P(x^{\langle 2\rangle }|x^{\langle 1\rangle })$。依此类推。最后一个输出是句子在这个地方结束的概率。</p>
<p>现在对于一个句子，它出现在语言中的概率可以根据将句子送进这个网络中产生的输出计算。比如句子是 this is a dog，则送入网络，第一次没有输入，第一个输出将给出语言中所有句子的第一个单词的概率分布，自然就知道单词是 this 的概率；第二次输入 this，第二次的输出给出了在已知了第一个单词是 this 的情况下，第二个单词的概率分布，自然就知道了单词是 is 的概率。类推即可。最后按照贝叶斯公式全部乘起来就得到了句子出现在语言中的概率。</p>
<h3 id="Sampling-novel-sequences"><a href="#Sampling-novel-sequences" class="headerlink" title="Sampling novel sequences"></a>Sampling novel sequences</h3><h4 id="生成文本"><a href="#生成文本" class="headerlink" title="生成文本"></a>生成文本</h4><p>使用建好模型的语言可以生成文本。还是用上面那个 RNN。第一步提供空输入，网络会返回第一个单词的概率分布。按照这个概率分布随机选择一个单词，将这个词作为生成的文本的第一个单词。然后构建这个词的 one-hot 向量后作为输入的第二个元素送入网络，则网络会给出在第一个单词正是该单词的情况下，第二个单词的概率分布。不断这样做下去。</p>
<p><strong>何时终止</strong>：如果 <code>&lt;EOS&gt;</code> 在字典中，可以在生成该 token 时终止，或者提前选好句子长度。</p>
<p>如果遇见了 <code>&lt;UNK&gt;</code>，可以不作处理或者重新选取单词。</p>
<h4 id="使用字符作为-token"><a href="#使用字符作为-token" class="headerlink" title="使用字符作为 token"></a>使用字符作为 token</h4><p>除了将单词作为 token 构建序列外也可以将字符作为 token 构建序列。</p>
<p><strong>好处</strong>：不用操心 <code>&lt;UNK&gt;</code>，因为可能出现的字符必然是有限的。</p>
<p><strong>坏处</strong>：序列的长度很长，计算量大。</p>
<h3 id="Vanishing-gradients-with-RNNs"><a href="#Vanishing-gradients-with-RNNs" class="headerlink" title="Vanishing gradients with RNNs"></a>Vanishing gradients with RNNs</h3><h4 id="简单-RNN-的问题"><a href="#简单-RNN-的问题" class="headerlink" title="简单 RNN 的问题"></a>简单 RNN 的问题</h4><p>因为序列的每个元素都会走一遍网络，所以网络可以在序列上“展开”。展开后会发现一层 RNN 变成了序列元素那么多层，如果序列很长的话，展开后的网络就会很深，这会带来梯度消失或者爆炸的问题。这将导致网络在调整权重时主要靠的是最后几个元素的影响，前面的若干元素的影响难以考虑到。</p>
<p>此外，直观上可以看到，对于很长的序列而言，生成的序列中的每个元素都主要依赖于它前面的几个元素，而更加前面的信息则容易丢失。这表明简单的 RNN 无法很好地处理对长距离的依赖。</p>
<h4 id="梯度爆炸的解决方案"><a href="#梯度爆炸的解决方案" class="headerlink" title="梯度爆炸的解决方案"></a>梯度爆炸的解决方案</h4><p>对于梯度爆炸有个简单的解决方案，就是预先设定一个阈值。如果梯度绝对值大于该阈值就在此将其截断，使得梯度绝对值总是不会超过阈值。</p>
<h3 id="Gated-Recurrent-Unit-GRU"><a href="#Gated-Recurrent-Unit-GRU" class="headerlink" title="Gated Recurrent Unit (GRU)"></a>Gated Recurrent Unit (GRU)</h3><p>GRU 通过在网络中引入负责记忆的单元来解决长距离的依赖问题。想法是在计算 $a^{\langle t \rangle}$ 的过程中，有时候适当保存 $a^{\langle t \rangle}$ 中的若干分量，在处理接下来的一些元素时保持这些分量不变，直到某个特定的时刻再允许其变动。此时的 $a^{\langle t \rangle}$ 在处理同一个序列的不同元素的过程中变成了某种像寄存器一样的东西，因此使用新的记号 $c^{\langle t \rangle}$ 来表示它（c 代表 cell）。此外，还需要引入一个量控制 $c^{\langle t \rangle}$ 是否不变，这个量为 $\Gamma_u$（u 代表 update）。</p>
<h4 id="简化版-GRU"><a href="#简化版-GRU" class="headerlink" title="简化版 GRU"></a>简化版 GRU</h4><p>在简单的 RNN 中，有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
a^{\langle t\rangle } &= g_1(W_{a}[a^{\langle t-1\rangle }, x^{\langle t\rangle }] + b_a) \\
y^{\langle t\rangle } &= g_2(W_{y}a^{\langle t\rangle } + b_y)
\end{aligned}</script><p>进行如下的改造：</p>
<ul>
<li>用 $c^{\langle t \rangle}$ 替换 $a^{\langle t \rangle}$。</li>
<li>在计算 $c^{\langle t \rangle}$ 时，首先计算出候选值 $\tilde{c}^{\langle t \rangle}$，同时更新 $\Gamma_u$ 的值。</li>
<li>使用 $\Gamma_u$ 决定 $c^{\langle t \rangle}$ 是被保持还是被候选值替代。</li>
</ul>
<p>公式变成：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\tilde c^{\langle t\rangle } &= g_1(W_{c}[c^{\langle t-1\rangle }, x^{\langle t\rangle }] + b_c) \\
\Gamma_u &= \sigma(W_{u}[c^{\langle t-1\rangle }, x^{\langle t\rangle }] + b_u) \\
c^{\langle t\rangle } &= \Gamma_u\ast \tilde{ c}^{\langle t \rangle} + (1 - \Gamma_u)\ast c^{\langle t - 1 \rangle} \\
y^{\langle t\rangle } &= g_2(W_{y}c^{\langle t\rangle } + b_y)
\end{aligned}</script><p>因为 $\Gamma_u$ 是经过 sigmoid 后得到的，因此大部分元素都接近 0 或者 1。$\Gamma_u$ 的大小和 $c^{\langle t \rangle}$ 是相同的，在第三个式子中，$\ast$ 进行的是逐元素乘法。考虑到 $\Gamma_u$ 的性质，它表示的意思就是让 $c^{\langle t - 1 \rangle}$ 中一部分元素更新为新的值，一部分元素保持原有值不变。从第二个式子可以看到每次计算 $\Gamma_u$ 时，只取决于上次的记忆单元的值和正在处理的元素，还比较合理。</p>
<p>这样建模的话，网络在处理序列中元素的过程中，每个序列中的元素都会更新一下 $c^{\langle t \rangle}$，但是因为 GRU 的机制，并非 $c^{\langle t \rangle}$ 的所有分量都被更新了，一个分量是否更新取决于它自己的值和读入的序列元素。这样一来，$c^{\langle t \rangle}$ 的某些分量就有可能在长序列中始终保持不变从而实现将信息存储起来，在经过了长序列后，这段序列前的一些重要信息依然得以保留，便实现了在长距离上的依赖关系。$\Gamma_u$ 可以看作是是否更新的“门”。$c^{\langle t \rangle}$ 中的每个分量可以看作是一个能够存储一个标量的寄存器，什么时候打开，什么时候关闭取决于 $c^{\langle t \rangle}$ 的内容和输入内容。</p>
<h4 id="完整版-GRU"><a href="#完整版-GRU" class="headerlink" title="完整版 GRU"></a>完整版 GRU</h4><p>上述 GRU 是一个简化版本。实际在计算候选值 $\tilde{c}^{\langle t \rangle}$ 时，我们还引入了一个控制 $c^{\langle t - 1 \rangle}$ 中的哪些分量会影响 $\tilde{c}^{\langle t \rangle}$ 的门 $\Gamma_r$（r 表示 relevance）。完整的 GRU 的公式为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\Gamma_r &= \sigma(W_{r}[c^{\langle t-1\rangle }, x^{\langle t\rangle }] + b_r) \\
\tilde c^{\langle t\rangle } &= g_1(W_{c}[\Gamma_r \ast c^{\langle t-1\rangle }, x^{\langle t\rangle }] + b_c) \\
\Gamma_u &= \sigma(W_{u}[c^{\langle t-1\rangle }, x^{\langle t\rangle }] + b_u) \\
c^{\langle t\rangle } &= \Gamma_u\ast \tilde{ c}^{\langle t \rangle} + (1 - \Gamma_u)\ast c^{\langle t - 1 \rangle} \\
y^{\langle t\rangle } &= g_2(W_{y}c^{\langle t\rangle } + b_y)
\end{aligned}</script><p>这样一来，一个完整的 GRU 单元需要维护 4 个权值和 4 个偏置。</p>
<h3 id="Long-Short-Term-Memory-LSTM"><a href="#Long-Short-Term-Memory-LSTM" class="headerlink" title="Long Short Term Memory (LSTM)"></a>Long Short Term Memory (LSTM)</h3><h4 id="常见版-LSTM"><a href="#常见版-LSTM" class="headerlink" title="常见版 LSTM"></a>常见版 LSTM</h4><p>常见版本的 LSTM 在 GRU 基础上的改动：</p>
<ul>
<li>GRU 中的记忆单元 $c^{\langle t \rangle}$ 和激活值是同一个量，但是 LSTM 将它们分开了。现在激活值就是激活值，不再负责存储；$c^{\langle t \rangle}$ 专门负责存储。</li>
<li>移除了负责关联的 $\Gamma_r$。</li>
<li>引入了两个新的门 $\Gamma_f$（forget）和 $\Gamma_o$（output）。</li>
</ul>
<p>公式如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\tilde c^{\langle t\rangle } &= g_1(W_{c}[a^{\langle t-1\rangle }, x^{\langle t\rangle }] + b_c) \\
\Gamma_u &= \sigma(W_{u}[a^{\langle t-1\rangle }, x^{\langle t\rangle }] + b_u) \\
\Gamma_f &= \sigma(W_{f}[a^{\langle t-1\rangle }, x^{\langle t\rangle }] + b_f) \\
\Gamma_o &= \sigma(W_{o}[a^{\langle t-1\rangle }, x^{\langle t\rangle }] + b_o) \\
c^{\langle t\rangle } &= \Gamma_u\ast \tilde{ c}^{\langle t \rangle} + \Gamma_f\ast c^{\langle t - 1 \rangle} \\
a^{\langle t \rangle} &= \Gamma_o\ast g_2(c^{\langle t \rangle}) \\
y^{\langle t\rangle } &= g_3(W_{y}a^{\langle t\rangle } + b_y)
\end{aligned}</script><p>逐行解释：</p>
<ul>
<li>根据上次的激活输出、这次的序列输入决定候选的记忆单元值。</li>
<li>根绝上次的激活输出、这次的序列输入决定 $\Gamma_u, \Gamma_f, \Gamma_o$。</li>
<li>根据 $\Gamma_u, \Gamma_f$ 决定记忆单元如何更新。其中 $\Gamma_u$ 负责更新，如果某个分量为零就表示不更新；$\Gamma_f$ 表示遗忘，如果某个分量为零就表示忘掉记忆单元原来的值。这是 $c^{\langle t - 1 \rangle}$ 和 $c^{\langle t \rangle}$ 的唯一联系。</li>
<li>根据当前记忆单元的值算出这次的激活输出。</li>
<li>根据这次的激活输出算出这次的序列输出。</li>
</ul>
<p>和 GRU 一样，$c^{\langle t \rangle}$ 可以看作是一组寄存器。LSTM 比 GRU 更加复杂，毕竟门更多。代价是 GRU 的运算更加简单。</p>
<h4 id="带-Peephole-connection-的-LSTM"><a href="#带-Peephole-connection-的-LSTM" class="headerlink" title="带 Peephole connection 的 LSTM"></a>带 Peephole connection 的 LSTM</h4><p>LSTM 最常见的一个变种是带有 peephole connection 的 LSTM。在上面的公式中，$\Gamma_u, \Gamma_f, \Gamma_o$ 只取决于上次的激活输出和这次的序列输入，而使用 peephole connection 后，它们还取决于上次的记忆单元的值。</p>
<p>注意在使用 peephole connection 时，$\Gamma_u, \Gamma_f, \Gamma_o$ 虽然会受到记忆单元的影响，但是记忆单元的一个分量只会影响到 $\Gamma_u, \Gamma_f, \Gamma_o$ 中对应位置的分量的值。换言之，记忆单元对 $\Gamma_u, \Gamma_f, \Gamma_o$ 在分量上是一对一的。</p>
<h3 id="Bidirectional-RNN"><a href="#Bidirectional-RNN" class="headerlink" title="Bidirectional RNN"></a>Bidirectional RNN</h3><p>上面的所有内容都是单向的 RNN。这种 RNN 在处理序列中的元素时，无法利用后面的元素的信息。使用双向的 RNN 可以解决这个问题。</p>
<p>想法：如果序列不是一个无限长的流，那么它总是有限的。用 RNN 正向处理一遍这个序列，会生成一个序列；如果反向处理一遍这个序列又可以生成一个序列。这两个序列结合起来得到的序列就是最终生成的序列。在最终的序列中，每个元素即受到了序列中在它前面的元素的影响，也受到了在它后面的元素的影响。</p>
<p>直观来看：</p>
<img src="/2019/03/23/Sequence-Models-笔记-1-Recurrent-Neural-Networks/DeepinScreenshot_select-area_20190328210024.png">
<p>注意即使正向反向都搞了一遍，整个图还是无环的。生成 $\hat y^{\langle t \rangle}$ 的方法可以使用 $\hat y^{\langle t \rangle} = W_y[\overrightarrow a^{\langle t \rangle}, \overleftarrow a^{\langle t \rangle}] + b_y$来计算。</p>
<p>在 NLP 任务中常用的套路是双向 LSTM。</p>
<h3 id="Deep-RNNs"><a href="#Deep-RNNs" class="headerlink" title="Deep RNNs"></a>Deep RNNs</h3><p>一层 RNN 可以将一个序列映射到另一个序列，得到的序列可以继续通过新的 RNN。这样做就可以将多层 RNN 叠起来得到一个复杂的网络。</p>
<p>因为 RNN 可以在序列上展开，所以一层 RNN 的计算量就比较大了，这也是为什么三层的 RNN 就算比较深的 RNN 网络了。</p>
<p>一种常用的架构是让序列通过多层 RNN 后得到的序列中的每个元素都再通过一个独立的小神经网络。注意每个元素都是一个向量，通过神经网络后得到的还是一个向量。例如最开始的输入序列长度为 n，经过 RNN 后得到的序列长度为 m，让这 m 个元素（每个是一个向量）独立地经过 m 个不同的神经网络，就可以再次得到长度为 m 的向量序列。这个序列可以作为最后的输出（并且每个元素不一定等长/平权）。</p>
<h2 id="关于-RNN-的反向传播"><a href="#关于-RNN-的反向传播" class="headerlink" title="关于 RNN 的反向传播"></a>关于 RNN 的反向传播</h2><p>此处内容来自于作业。推导中一些细节被略去了。</p>
<h3 id="普通-RNN-的反向传播"><a href="#普通-RNN-的反向传播" class="headerlink" title="普通 RNN 的反向传播"></a>普通 RNN 的反向传播</h3><h4 id="一个时间步上的反向传播"><a href="#一个时间步上的反向传播" class="headerlink" title="一个时间步上的反向传播"></a>一个时间步上的反向传播</h4><p>首先考虑一个时间步上的情况。假设我们已经知道了 $\displaystyle{\frac{\partial J}{\partial a^{\langle t\rangle}}}$。</p>
<p>假设使用的激活函数是 $\tanh$，那么有 $\displaystyle{a^{\langle t \rangle} = \tanh(W_{aa} a^{\langle t - 1 \rangle} + W_{ax} x^{\langle t \rangle} + b_a)}$。我们知道：</p>
<ul>
<li>$\displaystyle{\frac{\partial \tanh x}{\partial x}=1 - \tanh^2(x)}$</li>
<li>$\displaystyle{\mathbf Z = \mathbf {XY}, \frac{\partial J}{\partial \mathbf Z} = \mathbf M \Rightarrow \frac{\partial J}{\partial \mathbf X} = \mathbf M\mathbf Y^T, \frac{\partial J}{\partial \mathbf Y}=\mathbf X^T \mathbf M}$</li>
</ul>
<p>于是有：</p>
<ul>
<li>$\displaystyle{\frac{\partial J}{\partial W_{aa}} = (1 - \tanh^2(W_{aa} a^{\langle t - 1 \rangle} + W_{ax} x^{\langle t \rangle} + b_a)){a^{\langle t - 1 \rangle}}^T}$</li>
<li>$\displaystyle{\frac{\partial J}{\partial a ^{\langle t - 1 \rangle}} = W_{aa}^T(1 - \tanh^2(W_{aa} a^{\langle t - 1 \rangle} + W_{ax} x^{\langle t \rangle} + b_a))}$</li>
<li>$\displaystyle{\frac{\partial J}{\partial W_{ax}} = (1 - \tanh^2(W_{aa} a^{\langle t - 1 \rangle} + W_{ax} x^{\langle t \rangle} + b_a)){x^{\langle t \rangle}}^T}$</li>
<li>$\displaystyle{\frac{\partial J}{\partial x ^{\langle t \rangle}} = W_{ax}^T(1 - \tanh^2(W_{aa} a^{\langle t - 1 \rangle} + W_{ax} x^{\langle t \rangle} + b_a))}$</li>
<li>$\displaystyle{\frac{\partial J}{\partial b_a} = 1 - \tanh^2(W_{aa} a^{\langle t - 1 \rangle} + W_{ax} x^{\langle t \rangle} + b_a)}$</li>
</ul>
<p><em>计算时触发了 boardcast，所以在计算 $\displaystyle{\frac{\partial J}{\partial b_a}}$ 时应当注意缩减一下维度。</em></p>
<p>一个问题是 $\displaystyle{\frac{\partial J}{\partial a^{\langle t\rangle}}}$ 是怎么知道的。不严谨地，因为 $\displaystyle{a ^{\langle t \rangle}}$ 会参与计算 $\displaystyle{y ^{\langle t \rangle}}$，而 $\displaystyle{y ^{\langle t \rangle}}$ 会参与计算损失函数。</p>
<h4 id="一个序列上的反向传播"><a href="#一个序列上的反向传播" class="headerlink" title="一个序列上的反向传播"></a>一个序列上的反向传播</h4><blockquote>
<p>$\displaystyle{a ^{\langle t \rangle}}$ 会参与计算 $\displaystyle{y ^{\langle t \rangle}}$，而 $\displaystyle{y ^{\langle t \rangle}}$ 会参与计算损失函数。</p>
</blockquote>
<p>似乎从这里可以知道对于一个序列而言，我们就已经知道了对每个元素的 $\displaystyle{\frac{\partial J}{\partial a^{\langle t\rangle}}}$。但是实际情况并非如此，因为除了最后一个元素 $\displaystyle{a ^{\langle T_x \rangle}}$ 以外，其它的 $\displaystyle{a ^{\langle t \rangle}}$ 还会影响后续的 $\displaystyle{a ^{\langle t \rangle}}$。所以我们只知道 $\displaystyle{\frac{\partial J}{\partial a^{\langle T_x\rangle}}}$，要反着往回进行计算。</p>
<p>首先对序列中的最后一个元素套用上一节的过程，可以得到 $\displaystyle{\frac{\partial J}{\partial x ^{\langle T_x \rangle}}, \frac{\partial J}{\partial a ^{\langle T_x - 1 \rangle}}, \frac{\partial J}{\partial W_{ax}}, \frac{\partial J}{\partial W_{aa}}, \frac{\partial J}{\partial b_a}}$。</p>
<p>然后按照序列的顺序倒着操作，对每个元素套用上一节的过程。不过，在给定输入 $\displaystyle{\frac{\partial J}{\partial a^{\langle t\rangle}}}$ 时，它是由两部分组成的，一个是来自后一个序列得到的 $\displaystyle{\left(\frac{\partial J}{\partial a ^{\langle (t + 1) - 1 \rangle}}\right)_1}$，一个是从计算 $\displaystyle{y ^{\langle t \rangle}}$ 这条路径得到的 $\displaystyle{\left(\frac{\partial J}{\partial a ^{\langle t - 1 \rangle}}\right)_2}$。按照计算图求导的搞法，这两个量要加起来才能作为真实的输入。</p>
<p>当对整个序列全部执行了上述过程后，会得到多个不同的 $\displaystyle{\frac{\partial J}{\partial W_{ax}}, \frac{\partial J}{\partial W_{aa}}, \frac{\partial J}{\partial b_a}}$。还是按照计算图的搞法，不同的 $\displaystyle{\frac{\partial J}{\partial W_{ax}}}$ 加起来才是最后真实的 $\displaystyle{\frac{\partial J}{\partial W_{ax}}}$（因为在计算过程中一共走了 $\displaystyle{T_x}$ 条影响最终结果的路径）。$\displaystyle{W_{aa}, b_a}$ 相同。</p>
<p>此外还会得到对整个序列的 $\displaystyle{\frac{\partial J}{\partial x ^{\langle t \rangle}}}$，由于可能是多层 RNN 的复合，所以这个量是有意义的。将它们放在一个张量中作为对输入的导数。</p>
<h3 id="LSTM-的反向传播"><a href="#LSTM-的反向传播" class="headerlink" title="LSTM 的反向传播"></a>LSTM 的反向传播</h3><p>实在是推不动，还是算了吧。</p>

      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag"># 机器学习</a>
          
            <a href="/tags/深度学习/" rel="tag"># 深度学习</a>
          
            <a href="/tags/RNN/" rel="tag"># RNN</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/03/18/形式化验证课程笔记-4/" rel="next" title="形式化验证课程笔记 4">
                <i class="fa fa-chevron-left"></i> 形式化验证课程笔记 4
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/03/23/Sequence-Models-笔记-2-Natural-Language-Processing-Word-Embeddings/" rel="prev" title="Sequence Models 笔记 2 Natural Language Processing & Word Embeddings">
                Sequence Models 笔记 2 Natural Language Processing & Word Embeddings <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Zhang Yang</p>
              <div class="site-description motion-element" itemprop="description"></div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">119</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">20</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/dimlight1998" title="GitHub &rarr; https://github.com/dimlight1998" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="mailto:dimlight1998@gmail.com" title="E-Mail &rarr; mailto:dimlight1998@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
            </div>
          

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Recurrent-Neural-Networks"><span class="nav-number">1.</span> <span class="nav-text">Recurrent Neural Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Recurrent-Neural-Networks-1"><span class="nav-number">1.1.</span> <span class="nav-text">Recurrent Neural Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Why-sequence-models"><span class="nav-number">1.1.1.</span> <span class="nav-text">Why sequence models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Notation"><span class="nav-number">1.1.2.</span> <span class="nav-text">Notation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#序列相关的记号"><span class="nav-number">1.1.2.1.</span> <span class="nav-text">序列相关的记号</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#序列的表示"><span class="nav-number">1.1.2.2.</span> <span class="nav-text">序列的表示</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Recurrent-Neural-Network-Model"><span class="nav-number">1.1.3.</span> <span class="nav-text">Recurrent Neural Network Model</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#为何不使用普通网络"><span class="nav-number">1.1.3.1.</span> <span class="nav-text">为何不使用普通网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#递归神经网络"><span class="nav-number">1.1.3.2.</span> <span class="nav-text">递归神经网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#简单递归神经网络的问题"><span class="nav-number">1.1.3.3.</span> <span class="nav-text">简单递归神经网络的问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#前向传播"><span class="nav-number">1.1.3.4.</span> <span class="nav-text">前向传播</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Backpropagation-through-time"><span class="nav-number">1.1.4.</span> <span class="nav-text">Backpropagation through time</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#损失函数"><span class="nav-number">1.1.4.1.</span> <span class="nav-text">损失函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#反向传播"><span class="nav-number">1.1.4.2.</span> <span class="nav-text">反向传播</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Different-types-of-RNNs"><span class="nav-number">1.1.5.</span> <span class="nav-text">Different types of RNNs</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Many-to-many"><span class="nav-number">1.1.5.1.</span> <span class="nav-text">Many-to-many</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Many-to-one"><span class="nav-number">1.1.5.2.</span> <span class="nav-text">Many-to-one</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#One-to-one"><span class="nav-number">1.1.5.3.</span> <span class="nav-text">One-to-one</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#One-to-many"><span class="nav-number">1.1.5.4.</span> <span class="nav-text">One-to-many</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Many-to-many-with-different-length"><span class="nav-number">1.1.5.5.</span> <span class="nav-text">Many-to-many with different length</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Language-model-and-sequence-generation"><span class="nav-number">1.1.6.</span> <span class="nav-text">Language model and sequence generation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#什么是语言建模"><span class="nav-number">1.1.6.1.</span> <span class="nav-text">什么是语言建模</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#如何对语言建模"><span class="nav-number">1.1.6.2.</span> <span class="nav-text">如何对语言建模</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Sampling-novel-sequences"><span class="nav-number">1.1.7.</span> <span class="nav-text">Sampling novel sequences</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#生成文本"><span class="nav-number">1.1.7.1.</span> <span class="nav-text">生成文本</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#使用字符作为-token"><span class="nav-number">1.1.7.2.</span> <span class="nav-text">使用字符作为 token</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Vanishing-gradients-with-RNNs"><span class="nav-number">1.1.8.</span> <span class="nav-text">Vanishing gradients with RNNs</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#简单-RNN-的问题"><span class="nav-number">1.1.8.1.</span> <span class="nav-text">简单 RNN 的问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#梯度爆炸的解决方案"><span class="nav-number">1.1.8.2.</span> <span class="nav-text">梯度爆炸的解决方案</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Gated-Recurrent-Unit-GRU"><span class="nav-number">1.1.9.</span> <span class="nav-text">Gated Recurrent Unit (GRU)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#简化版-GRU"><span class="nav-number">1.1.9.1.</span> <span class="nav-text">简化版 GRU</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#完整版-GRU"><span class="nav-number">1.1.9.2.</span> <span class="nav-text">完整版 GRU</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Long-Short-Term-Memory-LSTM"><span class="nav-number">1.1.10.</span> <span class="nav-text">Long Short Term Memory (LSTM)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#常见版-LSTM"><span class="nav-number">1.1.10.1.</span> <span class="nav-text">常见版 LSTM</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#带-Peephole-connection-的-LSTM"><span class="nav-number">1.1.10.2.</span> <span class="nav-text">带 Peephole connection 的 LSTM</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Bidirectional-RNN"><span class="nav-number">1.1.11.</span> <span class="nav-text">Bidirectional RNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Deep-RNNs"><span class="nav-number">1.1.12.</span> <span class="nav-text">Deep RNNs</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#关于-RNN-的反向传播"><span class="nav-number">1.2.</span> <span class="nav-text">关于 RNN 的反向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#普通-RNN-的反向传播"><span class="nav-number">1.2.1.</span> <span class="nav-text">普通 RNN 的反向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#一个时间步上的反向传播"><span class="nav-number">1.2.1.1.</span> <span class="nav-text">一个时间步上的反向传播</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#一个序列上的反向传播"><span class="nav-number">1.2.1.2.</span> <span class="nav-text">一个序列上的反向传播</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LSTM-的反向传播"><span class="nav-number">1.2.2.</span> <span class="nav-text">LSTM 的反向传播</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhang Yang</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> v7.0.1</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/src/utils.js?v=7.0.1"></script>

  <script src="/js/src/motion.js?v=7.0.1"></script>



  
  


  <script src="/js/src/schemes/muse.js?v=7.0.1"></script>




  
  <script src="/js/src/scrollspy.js?v=7.0.1"></script>
<script src="/js/src/post-details.js?v=7.0.1"></script>



  


  <script src="/js/src/next-boot.js?v=7.0.1"></script>


  

  

  

  


  


  
  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('3');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<style>
.MathJax_Display {
  overflow-x: scroll;
  overflow-y: hidden;
}
</style>

    
  


  

  

  

  

  

  

  

  

  

</body>
</html>
